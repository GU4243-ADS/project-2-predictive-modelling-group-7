---
title: "R Notebook"
output: html_notebook
---

```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

```

```{r}
experiment_dir <- "../data/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep = "")
img_test_dir <- paste(experiment_dir, "test/", sep = "")

```

```{r}
library(tidyverse)
label <- read_csv(paste('../data/train_label.txt', sep=''),
                   col_names = c('label'))

label <-ifelse(label$label == "dog", 1, 0) # binary classification, dog = 1, cat = 0. 

```

```{r}
library(gbm)
hog <- read.csv("../output/hog_features.csv")

# sift <- read.csv("../pets/train-features")

hog$label <- label

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(hog$label, SplitRatio = 0.8)
training_set = subset(hog, split == TRUE)
label_set = subset(hog$label, split == TRUE)
test_set = subset(hog, split == FALSE)

tm_train<- system.time(gbm_train <- gbm.fit(x = training_set[-55],
                                  y = label_set, 
                                  n.trees = 1000,
                                  distribution = "bernoulli",
                                  interaction.depth = 3,
                                  bag.fraction = 0.5,
                                  verbose = FALSE))

# Predicting the Test set results
gbm_pred_hog <- predict(gbm_train, newdata = test_set[-55], n.trees = 1, distribution = "bernoulli", interaction.depth = 3)
mean(gbm_pred_hog)
```


```{r}
library(gbm)
sift <- read.csv("../output/sift_features.csv")

# sift <- read.csv("../pets/train-features")

sift$label <- label

# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(sift$label, SplitRatio = 0.8)
training_sift = subset(sift, split == TRUE)
label_sift = subset(sift$label, split == TRUE)
test_sift = subset(sift, split == FALSE)

tm_train<- system.time(gbm_sift <- gbm.fit(x = training_sift[-100],
                                  y = label_sift, 
                                  n.trees = 1000,
                                  distribution = "bernoulli",
                                  interaction.depth = 3,
                                  bag.fraction = 0.5,
                                  verbose = FALSE))

# Predicting the Test set results
gbm_pred_sift <- predict(gbm_sift, newdata = test_sift[-100], n.trees = 1, distribution = "bernoulli", interaction.depth = 3)
mean(gbm_pred_sift)
```

```{r}
# Making the Confusion Matrix
cm = table(test_set[, 55], gbm_pred_hog)

# Applying k-Fold Cross Validation
# install.packages('caret')
library(caret)

folds = createFolds(training_set$label, k = 10)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  gbm_train <- gbm.fit(x = training_fold[-55],
                                  y = training_fold$label, 
                                  n.trees = 1000,
                                  distribution = "bernoulli",
                                  interaction.depth = 3,
                                  bag.fraction = 0.5,
                                  verbose = FALSE)

  y_pred = predict(gbm_train, newdata = test_fold[-55], n.trees = 1000)
  y_pred = (y_pred >= 0.5)
  cm = table(test_fold[, 55], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
accuracy = mean(as.numeric(cv))
```
```{r}
accuracy
```

```{r}
# Making the Confusion Matrix
cm_sift = table(test_sift[, 100], gbm_pred_sift)

# Applying k-Fold Cross Validation
# install.packages('caret')
folds_sift = createFolds(training_sift$label, k = 10)
cv_sift = lapply(folds_sift, function(x) {
  training_fold_sift = training_sift[-x, ]
  test_fold_sift = training_sift[x, ]
  gbm_sift = gbm.fit(x = training_sift[-100],
                                  y = label_sift, 
                                  n.trees = 1000,
                                  distribution = "bernoulli",
                                  interaction.depth = 3,
                                  bag.fraction = 0.5,
                                  verbose = FALSE)

  pred_sift = predict(gbm_sift, newdata = test_fold_sift[-55], n.trees = 1, 
                   distribution = "bernoulli", interaction.depth = 3)
  pred_sift = (pred_sift >= 0.5)
  cm_sift = table(test_fold_sift[, 55], pred_sift)
  accuracy = (cm_sift[1,1] + cm_sift[2,2]) / (cm_sift[1,1] + cm_sift[2,2] + cm_sift[1,2] + cm[2,1])
  return(accuracy_sift)
})
accuracy_sift = mean(as.numeric(cv_sift))
```


```{r}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")

```

